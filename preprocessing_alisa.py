# -*- coding: utf-8 -*-
"""preprocessing Alisa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yVPcOJ_31UKE-QA_BFzwInyReDl_iaqW
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model
import statsmodels.api as sm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import warnings
from warnings import filterwarnings 
warnings.filterwarnings("ignore", category=DeprecationWarning) 
filterwarnings('ignore')
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV

df = pd.read_excel('hospital_deaths_train.xlsx')

def preprocess_data(df):
    #drop columns with 60% nulls
    a = (df.isnull().sum()/df.shape[0]*100).sort_values(ascending =False)
    col_to_drop = a[(a)>60].keys()
    output_df = df.drop(col_to_drop, axis=1) 


    # Identify potential outliers for each column
    outliers = {}
    for col in output_df.columns:
      q1 = output_df[col].quantile(0.05)
      q3 = output_df[col].quantile(0.95)
      iqr = q3 - q1
      upper_lim = q3 + 1.5 * iqr
      lower_lim = q1 - 1.5 * iqr
      outliers[col] = output_df.loc[(output_df[col] < lower_lim) | (output_df[col] > upper_lim), col]
      output_df.loc[(output_df[col] < lower_lim) | (output_df[col] > upper_lim), col] = np.nan
    
    #fill missing values with medians
    median_dict = output_df.median()
    for col in output_df.columns:
      median_value = median_dict[col]
      output_df[col].fillna(median_value, inplace=True)

    #splitting data
    xTrain, xTest, yTrain, yTest = train_test_split(output_df[output_df.columns.difference(['In-hospital_death'])],
                                                output_df['In-hospital_death'], test_size=0.2, random_state=78,stratify=output_df['In-hospital_death'])


    #scaling
    scaler = MinMaxScaler().fit(xTrain[xTrain.columns])  
    xTrain[xTrain.columns] = scaler.transform(xTrain[xTrain.columns])
    xTest[xTest.columns] = scaler.transform(xTest[xTest.columns])

  
    return xTrain , xTest ,yTrain , yTest

xTrain = preprocess_data(df)[0]
xTest = preprocess_data(df)[1]
yTrain = preprocess_data(df)[2]
yTest = preprocess_data(df)[3]

lasso=Lasso(random_state=78) 
params={'alpha':[1e-5,1e-4,1e-3,1e-2,0.1, 0.2, 0.3, 0.4, 0.5, 1, 2, 3, 4, 5, 10,20,30,40,50,100, 200, 300, 400,500]}
Regressor=GridSearchCV(lasso,params,scoring='neg_mean_squared_error',cv=10)
Regressor.fit(xTrain, yTrain)
print('best parameter: ', Regressor.best_params_)
print('best score: ', -Regressor.best_score_)

from sklearn.feature_selection import SelectFromModel
feature_sel_model = SelectFromModel(Lasso(alpha=0.001, random_state=78)) 
feature_sel_model.fit(xTrain, yTrain)
#list of the selected features
selected_feat = xTrain.columns[(feature_sel_model.get_support())]
# let's print some stats
print('total features: {}'.format((xTrain.shape[1])))
print('selected features: {}'.format(len(selected_feat)))
print('features with coefficients shrank to zero: {}'.format(
    np.sum(feature_sel_model.estimator_.coef_ == 0)))

selected_feat

# def diff(xTrain , XTest)
#     # here I overcome the problem when initial columns and structure of test dataset would be the same as the train dataset,
#     feature_difference = set(selected_feat) - set(XTest.columns())
#     # selected_feat list I got as a result of feature selection , I dont know how to use to it in this function.
#     feature_difference_df = pd.DataFrame(data=np.zeros((xTrain.shape[0], len(feature_difference))),
#                                      columns=list(feature_difference))
#     test = XTest.join(feature_difference_df)
#     for column in test.columns:
#         if column not in selected_feat:
#             test.drop([column], axis=1, inplace=True)
#     test = test.reindex(sorted(test.columns), axis=1)
#     return test

knn = KNeighborsClassifier()
param_grid = {'n_neighbors': [1,2,3,4,5,6, 7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,25,30], 'weights': ['uniform', 'distance']}

# Perform Grid Search with cross-validation
grid_search = GridSearchCV(knn, param_grid, cv=5)
grid_search.fit(xTrain, yTrain)

# Get best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Train the model with best parameters
best_k = best_params['n_neighbors']
best_weights = best_params['weights']
knn_best = KNeighborsClassifier(n_neighbors=best_k, weights=best_weights)
knn_best.fit(xTrain, yTrain)

# Make predictions
y_pred = knn_best.predict(xTest)

# Calculate accuracy
accuracy = accuracy_score(yTest, y_pred)
print('Accuracy:', accuracy)

best_params