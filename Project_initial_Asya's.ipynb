{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.impute import KNNImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anomaly Detection with OneClassSVM\n",
    "def anomaly_detection(series_obj):\n",
    "    svm = OneClassSVM()\n",
    "    series_obj = series_obj.values.reshape(-1,1)\n",
    "    # define the hyperparameters to search over\n",
    "    param_grid = {\n",
    "        'nu': [0.01, 0.05, 0.15, 0.2, 0.3, 0.35, 0.5 ], \n",
    "        'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto']}\n",
    "\n",
    "    gs = GridSearchCV(svm, param_grid=param_grid, scoring='roc_auc',cv=5)\n",
    "\n",
    "\n",
    "    #gs = GridSearchCV(OneClassSVM, {'kernel':['rbf',], 'gamma'})  # kernel,C,gamma\n",
    "    gs.fit(series_obj)\n",
    "    #model = OneClassSVM(kernel = 'rbf', gamma = 0.01).fit(X)\n",
    "    gs.best_estimator_\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    y_pred = gs.predict(series_obj)\n",
    "    \n",
    "\n",
    "    anomaly_val = np.where(y_pred==-1)\n",
    "    return anomaly_val[0]\n",
    "    #return gs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulizaion with histograms\n",
    "import matplotlib.pyplot as plt\n",
    "print(df['Height'].describe())\n",
    "plt.hist(df['Height'], bins='auto')  # arguments are passed to np.histogram\n",
    "\n",
    "plt.show()\n",
    "df['Height'] = df['Height'].drop(anomaly_detection(df['Height']))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['Height'], bins='auto')  # arguments are passed to np.histogram\n",
    "\n",
    "plt.show()\n",
    "print(df['Height'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fill nan values with KNN Inputer\n",
    "\n",
    "def filling_knn(series_obj):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X = series_obj .values.reshape(-1, 1)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    series_imputed = pd.Series(X_imputed.flatten(), index=series_obj.index)\n",
    "    return series_imputed\n",
    "print(filling_knn(df['Weight']).describe())\n",
    "\n",
    "# returns imputted data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "class model:\n",
    "    def __init__(self):\n",
    "        self.models = [KNeighborsClassifier(), DecisionTreeClassifier(), LogisticRegression()]\n",
    "        self.ensemble = BaggingClassifier()\n",
    "        self.random_state = 42\n",
    "        self.model = ...\n",
    "    def fit(self, X_train, y_train):\n",
    "        for model in self.models:\n",
    "            if model = KNeighborsClassifier():\n",
    "                param_grid  = {'n_neighbors':range(1,10), 'weights':['uniform', 'distance'], 'p':[1,2]}\n",
    "            elif model = DecisionTreeClassifier():\n",
    "                param_grid = {'criterion': ['gini', 'entropy'],\n",
    "                            'max_depth': [2, 3, 4, 5]}\n",
    "            elif model = LogisticRegression()\n",
    "                param_grid = {'C': [0.01, 0.1,1,2,4,5,6,8,9,10],\n",
    "                                'penalty': ['l1', 'l2'],\n",
    "                                'solver': ['lbfgs', 'liblinear', 'sag'],\n",
    "                                'max_iter': [100, 500, 1000]}\n",
    "            Model = (model,param_grid=param_grid, cv=5)\n",
    "            bag_clf = BaggingClassifier(Model, oob_score=True, self.random_state = 42).fit(X_train)\n",
    "            param_grid = {'n_estimators': [10, 50, 100, 200, 500]}\n",
    "            gs = GridSearchCV(bag_clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "            gs.fit(X_train, y_train)\n",
    "            \n",
    "    def predict(self, X):  \n",
    "        return self.model.predict_probas(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from preprocessor import Preprocessor\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--data_path\")\n",
    "parser.add_argument(\"--inference\")\n",
    "path = args.data_path\n",
    "test  = args.inference\n",
    "class Pipeline:\n",
    "    def __init__(self, path):\n",
    "        self.model = Model()\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.path = path\n",
    "    def run(self, X, test=False):\n",
    "        if test:\n",
    "            X = Preprocessing()\n",
    "            y = \n",
    "            self.Model().predict()\n",
    "            \n",
    "        # load preprocessor and model for testing\n",
    "        # save results to predictions.json file \n",
    "        else:\n",
    "        self.Model().fit(X)\n",
    "        # call preprocessor and model for training\n",
    "        # save preprocessor and model for future testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline().run()\n",
    "import json\n",
    "\n",
    "# Create a dictionary with the data to save\n",
    "data = {\"predict_probas\": ... ,\n",
    "    \"threshold\": ... }\n",
    "\n",
    "# Open the JSON file in write mode and write the data to it\n",
    "with open(\"predictions.json\", \"w\") as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the existing data from the file\n",
    "with open(\"predictions.json\", \"r\") as infile:\n",
    "    data = json.load(infile)\n",
    "if predict_probas > threshold:\n",
    "    predict_probas = 1\n",
    "else:\n",
    "    predict_probas = 0\n",
    "\n",
    "\n",
    "# Write the updated data back to the file\n",
    "with open(\"predictions.json\", \"w\") as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
